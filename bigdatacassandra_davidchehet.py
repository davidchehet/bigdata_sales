# -*- coding: utf-8 -*-
"""BigDataCassandra_DavidChehet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_6XDakK6CwCg08AhNFby13mIwd-PHLQu

# **Config**
"""

!pip install cassandra-driver

from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
import json

cloud_config= {
  'secure_connect_bundle': 'secure-connect-bigdata-sales.zip'
}

with open("bigdata_sales-token.json") as f:
    secrets = json.load(f)

CLIENT_ID = secrets["clientId"]
CLIENT_SECRET = secrets["secret"]

auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
session = cluster.connect()

if session:
  print('Connected!')
else:
  print("An error occurred.")



"""# **Bronze Data**"""

session.execute("""
    CREATE TABLE IF NOT EXISTS sales.bronze (
      region text,
      country text,
      item_type text,
      sales_channel text,
      order_priority text,
      order_date text,
      order_id int PRIMARY KEY,
      ship_date text,
      units_sold int,
      unit_price float,
      unit_cost float,
      total_revenue float,
      total_cost float,
      total_profit float
    )
""")

session = cluster.connect()

import pandas as pd
df = pd.read_csv('sales_100.csv')

df.head()

"""**Now we will add this raw data into the bronze table.**"""

#Fill database with CSV values
insert_query = session.prepare("""
INSERT INTO sales.bronze (region, country, item_type, sales_channel, order_priority, order_date, order_id, ship_date, units_sold,
unit_price, unit_cost, total_revenue, total_cost, total_profit)
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
""")

for index, row in df.iterrows():
  session.execute(insert_query, (row['Region'], row['Country'], row['Item Type'], row['Sales Channel'], row['Order Priority'],
                                 row['Order Date'], row['Order ID'], row['Ship Date'], row['UnitsSold'],
                                 row['UnitPrice'], row['UnitCost'], row['TotalRevenue'], row['TotalCost'], row['TotalProfit']))

print("Data inserted successfully.")

"""# **Silver Data**

Clean Date
"""

def clean_date(date):
  #X/X/XXXX -> XXXX-X-X
  if len(date) == 8:
    year = date[4:8]
    day = date[2]
    month = date[0]
    return f"{year}-{month}-{day}"
  #XX/XX/XXXX -> XXXX-XX-XX
  elif len(date) == 10:
    year = date[6:10]
    day = date[3:5]
    month = date[0:2]
    return f"{year}-{month}-{day}"
  elif len(date) == 9:
    #Universal Year
    year = date[5:9]
    #If the month is 2 digits
    if '/' in date[0:2]:
      month = date[0]
      day = date[2:4]
      return f"{year}-{month}-{day}"
    else:
      day = date[3]
      month = date[0:2]
      return f"{year}-{month}-{day}"

print(clean_date('6/25/2025'))
print(clean_date('10/25/2025'))
print(clean_date('6/5/2025'))
print(clean_date('10/5/2025'))

"""Clean Dates of DF"""

df['Order Date'] = df['Order Date'].apply(clean_date)
df['Ship Date'] = df['Ship Date'].apply(clean_date)
df.head()

"""I want to **binarize** the 'Sales_Channel' column to make the data easier to process. I will do this by changing the name of 'Sales_Channel' to 'Online'. It will be '1' if yes, and '0' if no. I think this makes sense to do to save memory especially if the dataset was bigger. I will also change the dataset of the column to a tinyint."""

dummy = pd.get_dummies(df['Sales Channel'])
dummy.head()

df = pd.concat((df, dummy), axis=1)

df

df = df.drop(['Online', 'Online', 'Offline', 'Offline'], axis=1)
df

df['Sales Channel'] = df['Sales Channel'].map({'Offline': 0, 'Online': 1})
df

df = df.rename(columns={'Sales Channel': 'Online Transaction'})
df

session.execute("""
    CREATE TABLE IF NOT EXISTS sales.silver (
      region text,
      country text,
      item_type text,
      online_transaction tinyint,
      order_priority text,
      order_date date,
      order_id int PRIMARY KEY,
      ship_date date,
      units_sold int,
      unit_price float,
      unit_cost float,
      total_revenue float,
      total_cost float,
      total_profit float
    )
""")

#Fill database with CSV values
insert_query = session.prepare("""
INSERT INTO sales.silver (region, country, item_type, online_transaction, order_priority, order_date, order_id, ship_date, units_sold,
unit_price, unit_cost, total_revenue, total_cost, total_profit)
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
""")

for index, row in df.iterrows():
  session.execute(insert_query, (row['Region'], row['Country'], row['Item Type'], row['Online Transaction'], row['Order Priority'],
                                 row['Order Date'], row['Order ID'], row['Ship Date'], row['UnitsSold'],
                                 row['UnitPrice'], row['UnitCost'], row['TotalRevenue'], row['TotalCost'], row['TotalProfit']))

print("Data inserted successfully.")

"""# **Gold Data Modeling**

Key part of Gold data is "the final layer of data transformations and data quality rules", according to a Databricks article on the Medallion architecture. I will interpret this as the place to remove unnecessary categorical fields that aren't needed, along with creating some features.


*   Profit Margin
*   Profit per Unit sold
*   Cost per Unit sold
"""

df.columns

"""I don't think we will need order_id anymore for business data, or order_priority."""

df = df.drop(['Order ID', 'Order Priority'], axis=1)

df['ProfitMargin'] = (df['TotalProfit'] / df['TotalRevenue'] * 100)

df['ProfitMargin'] = df['ProfitMargin'].round(2)

df['ProfitPerUnit'] = (df['TotalProfit'] / df['UnitsSold']).round(2)
df

df['CostPerUnit'] = (df['TotalCost'] / df['UnitsSold']).round(2)
df

df.columns

session.execute("""
    CREATE TABLE IF NOT EXISTS sales.gold (
      region text,
      country text,
      item_type text,
      online_transaction tinyint,
      order_date date,
      ship_date date,
      units_sold int,
      unit_price float,
      unit_cost float,
      total_revenue float,
      total_cost float,
      total_profit float,
      profit_margin float,
      profit_per_unit float,
      cost_per_unit float,
      PRIMARY KEY (country, order_date, item_type)
    ) WITH CLUSTERING ORDER BY (order_date DESC, item_type ASC)
""")

#Fill database with CSV values
insert_query = session.prepare("""
INSERT INTO sales.gold (region, country, item_type, online_transaction, order_date, ship_date, units_sold,
unit_price, unit_cost, total_revenue, total_cost, total_profit, profit_margin, profit_per_unit, cost_per_unit)
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
""")

for index, row in df.iterrows():
  session.execute(insert_query, (row['Region'], row['Country'], row['Item Type'], row['Online Transaction'], row['Order Date'], row['Ship Date'], row['UnitsSold'],
                                 row['UnitPrice'], row['UnitCost'], row['TotalRevenue'], row['TotalCost'], row['TotalProfit'], row['ProfitMargin'],
                                 row['ProfitPerUnit'], row['CostPerUnit']))

print("Data inserted successfully.")




